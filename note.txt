# Guidelines for contributing to the repository. Please, let's try to:
## write comments at the top of reasonably-sized chuncks of your code
## commit your work frequently
    ### allows faster communication among contributors to the code
    ### reduces the chances of losing work in the local repository
## maintain the namespace conventions
## add a comment that starts with '# temp -- ', when you are making a temporary change to the code, or hacking it just to try something for a moment. This makes it easy to notice or search hacky, temporary or extraordinary changes to the repository.
## avoid uncommented parameter values in arbitrary locations in the code. Try to define parameter values together at the top your code and comment them.
## not repeat code. If you need to do the same operation in multiple points, make your code a function and call it multiple times.
## maintain separate branches for major tasks, which will then be merged into the master branch
## keep a single file (main.py) for all functions for the moment. Write new files for executables (such as expl.py)

# Useful links
## Currently known exoplanets
## http://exoplanetarchive.ipac.caltech.edu/

# Useful information
To set up your environment variables, put
export TDGU_DATA_PATH=/path/to/your/folder/
in your ~/.bashrc file.
export $PYTHONPATH=$PYTOHPATH:/path/to/thefolderwhereyouhaveexopandctc

# Levels of data processing
1 - Raw light curve
2 - Phase-folded light curve
3 - Rebinning of the phased-folded light curve

## Data
### Kepler
### https://archive.stsci.edu/kepler/
### ETE-6 (TESS Simulated)
### https://archive.stsci.edu/tess/ete-6.html

## Baseline paper
## https://arxiv.org/pdf/1712.05044.pdf


# to-do-list
## implement a simple 1D CNN on 
## implement a 2 channel 1D CNN with FC layers
## implement local and global light curve representations
## get Kepler data from MAST
## Ensure the above nets perform well
## Channel off to individual projects

# diagnostic actions to take when the solution is not working
## data augmentation -- increase the number of (signal) training data samples by drawing from a previously trained generative model
## add batch normalization after FC or pool layers

# diagnostic plots to produce
## animation showing samples from the training data samples
## a Probabilistic Graphical Model (PGM) of your neural network
## Recall, precision and accuracy for the training and test data sets as a function of epoch index
## Recall vs. precision for a list of classification thresholds and the Area Under the Curve (AUC).
## The data distribution in intermediate representations (i.e., after each successive layer in the network)
## 2D projection of the feature space representation of the training data samples (e.g. via t-SNE, variational autoencoder or SOM)


# log
20180911
- moved some content from inside the constructor of gdatstrt into the loop, where we iterate over hyperparameters
- changed whchLayr to strglayr and ensured it only admits a single type of data struture (not int and string at the same time)
- moved utility functions into the same (main.py) file. The idea is to have the callable code in one file for the moment, and call it from various callables (such as expl.py).
- should unify Nets into main.py
- connected exop.py to CtC, got rid of data generation inside CtC
- added numblayr, numbdimslayr and fracdrop hyperparameters

